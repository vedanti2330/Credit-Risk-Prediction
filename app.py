{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1aabc6-4a7d-4f52-869d-966ec7731131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "import io # Required for st.info buffer\n",
    "\n",
    "# --- Configuration ---\n",
    "st.set_page_config(page_title=\"Credit Risk Prediction\", layout=\"wide\")\n",
    "st.set_option('deprecation.showPyplotGlobalUse', False) # Suppress Pyplot warning\n",
    "\n",
    "# --- Caching Functions ---\n",
    "@st.cache_data\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads the CSV data, handling potential errors.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while loading the data: {e}\")\n",
    "        return None\n",
    "\n",
    "@st.cache_data\n",
    "def preprocess_data(df_raw):\n",
    "    \"\"\"Applies preprocessing steps: imputation, dropping columns, encoding, scaling.\"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- Imputation ---\n",
    "    # Mean for roughly symmetric numeric columns\n",
    "    mean_cols = ['age', 'credit_amount', 'duration', 'income', 'interest_rate']\n",
    "    for col in mean_cols:\n",
    "        if col in df.columns:\n",
    "            mean_val = df[col].mean()\n",
    "            df[col] = df[col].fillna(mean_val)\n",
    "\n",
    "    # Median for skewed numeric column\n",
    "    median_cols = ['existing_loans_count']\n",
    "    for col in median_cols:\n",
    "         if col in df.columns:\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "\n",
    "    # Mode for categorical columns\n",
    "    mode_cols = ['job', 'marital_status', 'education_level', 'loan_type']\n",
    "    for col in mode_cols:\n",
    "         if col in df.columns:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "    # --- Drop Columns ---\n",
    "    df.drop(columns=['purpose', 'loan_eligibility'], inplace=True, errors='ignore')\n",
    "\n",
    "    # --- Drop Rows with Remaining NaNs ---\n",
    "    initial_rows = df.shape[0]\n",
    "    df.dropna(inplace=True)\n",
    "    rows_dropped = initial_rows - df.shape[0]\n",
    "\n",
    "    # --- Encoding ---\n",
    "    # Label Encode Target Variable ('default')\n",
    "    if 'default' in df.columns:\n",
    "        le_default = LabelEncoder()\n",
    "        df['default'] = le_default.fit_transform(df['default'])\n",
    "\n",
    "    # Label Encode 'sex'\n",
    "    if 'sex' in df.columns:\n",
    "        le_sex = LabelEncoder()\n",
    "        df['sex'] = le_sex.fit_transform(df['sex'])\n",
    "\n",
    "    # Separate Features (X) and Target (Y) before One-Hot Encoding\n",
    "    if 'default' not in df.columns:\n",
    "        st.error(\"Target column 'default' not found after initial processing.\")\n",
    "        return None, None, rows_dropped # Return None if target is missing\n",
    "\n",
    "    X_raw = df.drop(columns=['default'])\n",
    "    Y = df['default']\n",
    "\n",
    "    # One-Hot Encode remaining categorical features\n",
    "    X = pd.get_dummies(X_raw, drop_first=True).astype(int)\n",
    "\n",
    "    # --- Scaling Numeric Features ---\n",
    "    numeric_cols = ['age', 'credit_amount', 'duration', 'number_of_dependents', 'income',\n",
    "                    'existing_loans_count', 'credit_history_length', 'previous_defaults',\n",
    "                    'credit_score', 'installment_rate', 'interest_rate']\n",
    "    # Ensure only columns present in X are scaled\n",
    "    numeric_cols_in_x = [col for col in numeric_cols if col in X.columns]\n",
    "    if numeric_cols_in_x:\n",
    "        scaler = StandardScaler()\n",
    "        X[numeric_cols_in_x] = scaler.fit_transform(X[numeric_cols_in_x])\n",
    "\n",
    "    return X, Y, rows_dropped\n",
    "\n",
    "# --- Model Training and Tuning ---\n",
    "@st.cache_resource # Cache the trained model object\n",
    "def train_initial_model(X_train, Y_train):\n",
    "    \"\"\"Trains the initial Logistic Regression model.\"\"\"\n",
    "    logreg = LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga')\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    return logreg\n",
    "\n",
    "@st.cache_resource # Cache the grid search results and best model\n",
    "def tune_hyperparameters(X_train, Y_train):\n",
    "    \"\"\"Performs GridSearchCV for Logistic Regression.\"\"\"\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['saga'],\n",
    "        'max_iter': [2000]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=LogisticRegression(class_weight='balanced'),\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='roc_auc',\n",
    "                               cv=5,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=0) # Reduced verbosity for Streamlit\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    return grid_search\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"Plots a confusion matrix using seaborn.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    st.pyplot(fig)\n",
    "\n",
    "# --- Streamlit App ---\n",
    "def main():\n",
    "    st.title(\"üìä Credit Risk Prediction using Logistic Regression\")\n",
    "    st.markdown(\"This app demonstrates the process of building and evaluating a Logistic Regression model for credit risk prediction based on the provided dataset.\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    st.header(\"1. Data Loading\")\n",
    "    data_file = 'Credit_risk_data.csv'\n",
    "    df_raw = load_data(data_file)\n",
    "\n",
    "    if df_raw is not None:\n",
    "        st.subheader(\"Raw Data Sample\")\n",
    "        st.dataframe(df_raw.head())\n",
    "        st.write(f\"Original Data Shape: `{df_raw.shape}`\")\n",
    "\n",
    "        # --- 2. Initial Data Exploration ---\n",
    "        st.header(\"2. Initial Data Exploration\")\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.subheader(\"Data Types\")\n",
    "            buffer = io.StringIO()\n",
    "            df_raw.info(buf=buffer)\n",
    "            st.text(buffer.getvalue())\n",
    "\n",
    "            st.subheader(\"Missing Values (Before Imputation)\")\n",
    "            st.dataframe(df_raw.isna().sum().reset_index().rename(columns={0: 'Missing Count', 'index':'Column'}))\n",
    "\n",
    "        with col2:\n",
    "            st.subheader(\"Target Variable Distribution\")\n",
    "            fig, ax = plt.subplots()\n",
    "            df_raw['default'].value_counts().plot(kind='bar', ax=ax, color=['skyblue', 'salmon'])\n",
    "            ax.set_title(\"Distribution of 'default' Target Variable\")\n",
    "            ax.set_xlabel(\"Default Status\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            ax.tick_params(axis='x', rotation=0)\n",
    "            st.pyplot(fig)\n",
    "\n",
    "            st.subheader(\"Numerical Feature Distributions (Histograms)\")\n",
    "            hist_cols = [\"age\",\"credit_amount\",\"duration\",\"income\",\"existing_loans_count\",\"interest_rate\"]\n",
    "            fig_hist, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "            axes = axes.flatten() # Flatten to 1D array for easier iteration\n",
    "            for i, col in enumerate(hist_cols):\n",
    "                 if col in df_raw.columns:\n",
    "                    df_raw[[col]].hist(bins=20, edgecolor=\"black\", ax=axes[i])\n",
    "                    axes[i].set_title(col)\n",
    "            plt.tight_layout()\n",
    "            st.pyplot(fig_hist)\n",
    "\n",
    "\n",
    "        # --- 3. Data Preprocessing ---\n",
    "        st.header(\"3. Data Preprocessing\")\n",
    "        with st.spinner(\"Preprocessing data...\"):\n",
    "            X, Y, rows_dropped = preprocess_data(df_raw)\n",
    "\n",
    "        if X is None or Y is None:\n",
    "            st.stop() # Stop execution if preprocessing failed\n",
    "\n",
    "        st.success(\"Preprocessing complete!\")\n",
    "        st.write(f\"Rows dropped due to remaining NaNs after imputation: `{rows_dropped}`\")\n",
    "        st.write(f\"Data Shape after Preprocessing (Features X): `{X.shape}`\")\n",
    "        st.write(f\"Data Shape after Preprocessing (Target Y): `{Y.shape}`\")\n",
    "\n",
    "        st.subheader(\"Features after Encoding and Scaling (Sample)\")\n",
    "        st.dataframe(X.head())\n",
    "\n",
    "        # --- 4. Train/Test Split ---\n",
    "        st.header(\"4. Train/Test Split\")\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X, Y, test_size=0.2, random_state=42, stratify=Y\n",
    "        )\n",
    "        st.write(f\"Training Set Shape: X=`{X_train.shape}`, Y=`{Y_train.shape}`\")\n",
    "        st.write(f\"Testing Set Shape: X=`{X_test.shape}`, Y=`{Y_test.shape}`\")\n",
    "\n",
    "        # --- 5. Initial Model Training & Evaluation ---\n",
    "        st.header(\"5. Initial Logistic Regression Model\")\n",
    "        with st.spinner(\"Training initial model...\"):\n",
    "            logreg_initial = train_initial_model(X_train, Y_train)\n",
    "        st.success(\"Initial model trained!\")\n",
    "\n",
    "        st.subheader(\"Initial Model Evaluation (on Test Set)\")\n",
    "        Y_pred_initial = logreg_initial.predict(X_test)\n",
    "        Y_proba_initial = logreg_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        acc_initial = accuracy_score(Y_test, Y_pred_initial)\n",
    "        roc_auc_initial = roc_auc_score(Y_test, Y_proba_initial)\n",
    "        report_initial = classification_report(Y_test, Y_pred_initial, digits=3)\n",
    "\n",
    "        st.metric(\"Accuracy\", f\"{acc_initial:.3f}\")\n",
    "        st.metric(\"ROC AUC Score\", f\"{roc_auc_initial:.3f}\")\n",
    "        st.subheader(\"Classification Report\")\n",
    "        st.text(report_initial)\n",
    "        plot_confusion_matrix(Y_test, Y_pred_initial, \"Initial Model Confusion Matrix\")\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # --- 6. Hyperparameter Tuning ---\n",
    "        st.header(\"6. Hyperparameter Tuning (GridSearchCV)\")\n",
    "        with st.spinner(\"Performing Grid Search... (This might take a moment)\"):\n",
    "            grid_search = tune_hyperparameters(X_train, Y_train)\n",
    "        st.success(\"Grid Search complete!\")\n",
    "\n",
    "        st.subheader(\"Best Parameters Found\")\n",
    "        st.write(grid_search.best_params_)\n",
    "        best_logreg = grid_search.best_estimator_\n",
    "\n",
    "        # --- 7. Tuned Model Evaluation ---\n",
    "        st.header(\"7. Tuned Logistic Regression Model Evaluation\")\n",
    "        st.subheader(\"Tuned Model Evaluation (on Test Set)\")\n",
    "        Y_pred_tuned = best_logreg.predict(X_test)\n",
    "        Y_proba_tuned = best_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        acc_tuned = accuracy_score(Y_test, Y_pred_tuned)\n",
    "        roc_auc_tuned = roc_auc_score(Y_test, Y_proba_tuned)\n",
    "        report_tuned = classification_report(Y_test, Y_pred_tuned, digits=3)\n",
    "\n",
    "        st.metric(\"Tuned Model Accuracy\", f\"{acc_tuned:.3f}\")\n",
    "        st.metric(\"Tuned Model ROC AUC Score\", f\"{roc_auc_tuned:.3f}\")\n",
    "        st.subheader(\"Tuned Model Classification Report\")\n",
    "        st.text(report_tuned)\n",
    "        plot_confusion_matrix(Y_test, Y_pred_tuned, \"Tuned Model Confusion Matrix\")\n",
    "\n",
    "        st.divider()\n",
    "\n",
    "        # --- 8. Generalization Check ---\n",
    "        st.header(\"8. Model Generalization Check\")\n",
    "        Y_pred_train_tuned = best_logreg.predict(X_train)\n",
    "        train_accuracy_tuned = accuracy_score(Y_train, Y_pred_train_tuned)\n",
    "        test_accuracy_tuned = accuracy_score(Y_test, Y_pred_tuned) # Same as acc_tuned\n",
    "\n",
    "        st.write(f\"Tuned Model Train Accuracy: `{train_accuracy_tuned:.3f}`\")\n",
    "        st.write(f\"Tuned Model Test Accuracy: `{test_accuracy_tuned:.3f}`\")\n",
    "\n",
    "        # Determine generalization status based on the tuned model\n",
    "        if abs(train_accuracy_tuned - test_accuracy_tuned) < 0.05:\n",
    "            st.success(\"‚úÖ Good generalization (Train and Test accuracies are similar)\")\n",
    "        elif train_accuracy_tuned > test_accuracy_tuned + 0.1:\n",
    "            st.warning(\"‚ö†Ô∏è Overfitting may be present (Train accuracy significantly higher than Test).\")\n",
    "            st.markdown(\"\"\"\n",
    "                **Potential Solutions:**\n",
    "                * Use stronger regularization (adjust 'C' parameter).\n",
    "                * Perform feature selection/engineering.\n",
    "                * Gather more diverse training data.\n",
    "            \"\"\")\n",
    "        else: # Covers underfitting or other unexpected gaps\n",
    "            st.info(\"‚ùì Check model complexity or data representativeness (potential underfitting or other issues).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
